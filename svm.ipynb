{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7b3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Mild Impairment', 'Moderate Impairment', 'No Impairment', 'Very Mild Impairment']\n",
      "Train samples: 10240\n",
      "Test samples: 1279\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Paths ---\n",
    "train_dir = Path(\"../Combined Dataset/train\")\n",
    "test_dir  = Path(\"../Combined Dataset/test\")\n",
    "\n",
    "# --- Image transforms ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # ensure 1-channel MRI\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# --- Load datasets ---\n",
    "train_ds = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_ds  = datasets.ImageFolder(root=test_dir,  transform=transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# --- Print info ---\n",
    "print(\"Classes:\", train_ds.classes)\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Test samples:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f5dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and flattening data...\n",
      "Train shape: (10240, 16384), Test shape: (1279, 16384)\n",
      "\n",
      "Standardizing features...\n",
      "\n",
      "Applying PCA...\n",
      "PCA components: 200, Explained variance: 0.824\n",
      "\n",
      "Performing grid search with stratified 5-fold CV...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Extract images and labels from DataLoaders ---\n",
    "def extract_data(loader):\n",
    "    \"\"\"Convert PyTorch DataLoader to numpy arrays\"\"\"\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for imgs, labels in loader:\n",
    "        images_list.append(imgs.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "    images = np.concatenate(images_list, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return images, labels\n",
    "\n",
    "print(\"Extracting and flattening data...\")\n",
    "X_train, y_train = extract_data(train_loader)\n",
    "X_test, y_test = extract_data(test_loader)\n",
    "\n",
    "# Flatten 128x128 images to vectors\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  # (n_samples, 128*128)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Train shape: {X_train_flat.shape}, Test shape: {X_test_flat.shape}\")\n",
    "\n",
    "# --- Standardization ---\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "# --- PCA for dimensionality reduction ---\n",
    "print(\"\\nApplying PCA...\")\n",
    "pca = PCA(n_components=200, random_state=42)  # try 100-300 range\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"PCA components: {pca.n_components_}, Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# --- Grid Search with Stratified CV ---\n",
    "print(\"\\nPerforming grid search with stratified 5-fold CV...\")\n",
    "\n",
    "# Linear SVM\n",
    "param_grid_linear = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "grid_linear = GridSearchCV(\n",
    "    svm_linear, \n",
    "    param_grid_linear, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_linear.fit(X_train_pca, y_train)\n",
    "\n",
    "print(f\"\\nBest Linear SVM params: {grid_linear.best_params_}\")\n",
    "print(f\"Best CV macro F1: {grid_linear.best_score_:.4f}\")\n",
    "\n",
    "# RBF SVM\n",
    "param_grid_rbf = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "}\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "grid_rbf = GridSearchCV(\n",
    "    svm_rbf,\n",
    "    param_grid_rbf,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_rbf.fit(X_train_pca, y_train)\n",
    "\n",
    "print(f\"\\nBest RBF SVM params: {grid_rbf.best_params_}\")\n",
    "print(f\"Best CV macro F1: {grid_rbf.best_score_:.4f}\")\n",
    "\n",
    "# --- Select best model and evaluate on test set ---\n",
    "best_model = grid_linear if grid_linear.best_score_ > grid_rbf.best_score_ else grid_rbf\n",
    "model_name = \"Linear SVM\" if best_model == grid_linear else \"RBF SVM\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Best model: {model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_pca)\n",
    "\n",
    "# --- Evaluation metrics ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=train_ds.classes))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=train_ds.classes, \n",
    "            yticklabels=train_ds.classes)\n",
    "plt.title(f'Confusion Matrix - {model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Per-class confusion matrices (normalized) ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, class_name in enumerate(train_ds.classes):\n",
    "    cm_class = confusion_matrix(y_test == idx, y_pred == idx)\n",
    "    sns.heatmap(cm_class, annot=True, fmt='d', cmap='Greens', \n",
    "                ax=axes[idx], xticklabels=['Other', class_name], \n",
    "                yticklabels=['Other', class_name])\n",
    "    axes[idx].set_title(f'{class_name}')\n",
    "    axes[idx].set_ylabel('True')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS3000env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
